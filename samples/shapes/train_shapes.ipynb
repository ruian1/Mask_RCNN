{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "\n",
    "\n",
    "# Local path to trained weights file\n",
    "# COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "# if not os.path.exists(COCO_MODEL_PATH):\n",
    "#     utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_CHANNEL_COUNT            1\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   1]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.02\n",
      "LOSS_WEIGHTS                   {'mrcnn_mask_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'rpn_class_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAy4AAACnCAYAAAD35AgmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAADpdJREFUeJzt3X2sZHddx/HPF1oIikoJFoiQIChI\niQ8NVlCwVi2hoJaID5EIRK0GAyViIQqIiFBFkChN2voApYpClChUIqtoKQVbu1BLEwQjShCfeCjV\nplCthcLPP+ZMerm9u/fe3Zl7fmfO65Vs9s65s2d+5+7Zu+c935ndaq0FAACgZ3cZewEAAAC7ES4A\nAED3hAsAANA94QIAAHRPuAAAAN0TLgAAQPdmEy5V9aCqunzbtg8fw34OVdWpw8dPrKqbqqqG26+s\nqqftYR8vq6p/3bqeqjq1qq6uqndX1RVV9eBh+4OHbVdW1Tur6gFH2e9Dquq6qrqlqh67Zfurq+rw\n8OP5W7a/oKqurar3VtV5+/1aMK6quldVPf0In3t1VX3lih7nTn92AAAO2mzCZYWuSvKY4ePHJLku\nySO23P6bPezj4iTfuW3bx5Oc1Vo7PcmrkvzysP2ZSS5prZ2R5PeTPPso+/14kscl+ZNt2y9qrT06\nybcledIQOF+W5CeSLLf/dFV96R7WTj/uleRO4VJVd22tPae19qkR1gRrUVV3HXsNAIxLuGxTVRdX\n1dOr6i5V9faqetS2u1yVZDnN+MYkv5XksVV19yT3ba19dLfHaK19PMkXtm37RGvtM8PN25LcPnz8\nwSwuUJPkpCQ3VNXdq+qqqvq6qrrfMDE5qbX2v621/97h8f55+PkLw34/n+TWJB9Lco/hx61JPrfb\n2unKeUkeOUzjrq2q36uqtyb54WHbA6rqPlX1juH21VX10CQZ7vuaqnrbMIk7edh+XlX9XVW9Ydjn\ng7Y+YFU9cPg1Vww/r2Sqw/RV1SOq6pphMvwXVXXK8L3pbVX1pqp6yXC/D2/5Na+tqjOGj98+nKfv\nrapvHba9ZNt5/R1V9a7hfr+9nHYDMA8njL2AA/bIqrpyl/ucl+SKLKYn72itvWfb59+b5HVVdWKS\nlsWE5VVJPpDk2iQZ/tJ9+Q77fmlr7YqjPfgw9Tg/yTnDpsuTvL2qzkly9yTf0lq7bbh9aZKbkzyn\ntXbTLseVqvrRJB9ZxlVVHUryoSwC9vzW2md32wdd+Y0kp7TWzhwuCu/fWjs7SarqGcN9bk7yhNba\nZ6vqCUmen8WkLUk+2Fr7qap6YRYXhW9K8rQkpyX5kiQf2eExfz3Jy1prh6vqSUl+Psnz1nR8TMvj\nk1zaWvvdqrpLkrck+ZnW2jVV9Zo9/Pont9b+p6oenuSiJN81bL+ttXb2ECnvS3JGa+3mqvrNJN+T\n5M/XcCwAdGhu4XJda+3M5Y3a4T0urbX/q6pLk7wyyf2P8Pkbkjw5yfWttRuq6n5ZTGGuGu5zTZIz\n9ru4IYb+OMkrWmv/MGx+RZIXtdbeXFVPSfKrSZ7VWvtQVf1Lknu31v52D/s+M8mPJ/m+4fZDk/xA\nkgdnES7vqqrLWmv/ud91042dzoN7JbloOEfvluQzWz533fDzvyV5SJKvTvKB1trtST5dVf+4w/6+\nPsmvDU90n5Bk3+8TY2NdmuQXquoNSd6f5GuzeKInSd6TZKf35y3fH3iPJBdU1cOymAh/1Zb7LM/r\n+yR5UJI/G86/e2bxxAsct6o6N8kPJvlwa+0nx14P8+Mc3Ju5hcuuqur+WUw7XpZFJOz0pvWrkvxc\nkhcOtz+W5IeyCINjmrgMz1D+YZLLWmuXbf1UkhuHj29Icu/h/o9LcmKSG6vq7NbaW49yTI8ajucJ\nrbVbt+z3M62124b73JbFhQDT8dl88Z/hz+9wn6dmEdgvr6on5ovP57bl40ry0SSPqKoTsnj54MN2\n2N8Hk7y8tXZ9klTV3Y59+WyY21prz0uS4R9z+GSSb84iWk7L4j14SXLzENKfSvJNSf4gyVlJPt9a\n+/aqOiXJ1u9ny/P6xiymgN/bWrtleJwT13tIzEVr7cIkF469DubLObg3wmWLIR4uzeKlV4er6o+q\n6omttUPb7npVkucmOTzcvjrJk7J4udiuE5ehqn8kycOHv+CfkeTULF72cN+qemqSv2+tPTuLl439\nTlXdnkWoPGN4P8KvZPHSjNuTXF5V70vy6SRvTnJKFhegh1prv5TkkuGhLxueqXxua+264bXkh7O4\naH1na82zl9PyiSS3VtWfJjk5O08//irJG6vq9Cyi44haa5+sqjdmcaH5T0n+I4s42honz81igrOM\n3NdlEdzwlKr6sSyC+BNZfO96bVX9V+548iVZTLP/Oovz8YZh2zVJXjB8P7x6p5231lot/vXDtw4v\nG/tCkp/NYroDwAxUa233ewGzUFUnttY+V1VfnuT6JA9tre00yYE9G56M+ZrW2kvGXgsA02XiAmz1\n/Kr67iRfkeQXRQsA0AsTFwAAoHv+HxcAAKB7wgUAAOheF+9xeeA3nOn1ajPy7++/vMv/7foep57r\nPJyRW6+/0HnI6Ho8D52D89LjOZg4D+dmr+ehiQsAANA94QIAAHRPuAAAAN0TLgAAQPeECwAA0D3h\nAgAAdE+4AAAA3RMuAABA94QLAADQPeECAAB0T7gAAADdEy4AAED3hMsODr3+grGXALnp2gvHXgIA\nQDeEyzbLaBEvjGkZLeIFAGBBuGyxPVbEC2PYHiviBQBAuAAAABMgXAZHmq6YunCQjjRdMXUBAOZO\nuESc0AdxAgBwZMJlD4QNPRA2AMCczT5c9hol4oV12muUiBcAYK5mHy4AAED/Zh0u+52imLqwDvud\nopi6AABzNNtwESH0QIQAAOzNbMPlWAkeeiB4AIC5mWW4iA96ID4AAPZuluFyvIQPPRA+AMCczC5c\nVhUd4oXjsaroEC8AwFzMKlzEBj0QGwAA+zercAEAAKZpNuFi2kIPTFsAAI7NbMJlHcQQPRBDAMAc\nzCJc1hkY4oW9WmdgiBcAYNNtfLgIC3ogLAAAjs/Gh8tBEEf0QBwBAJtso8NFUNADQQEAcPw2OlwO\nkkiiByIJANhUGxsuQoIeCAkAgNXYyHAZK1rEEluNFS1iCQDYRBsZLmMSL/RAvAAAm2bjwkU40APh\nAACwWhsXLj0QT/RAPAEAm2SjwkUw0APBAACweieMvYAkefcNZ6xkP7esZC+rcej1F+SeZ73lTttP\nP/nKg18Ms3XTtRfmpNPOHXsZ7MOqwre33/edjqu3NQLQt42ZuNzyl98/9hLAtIUuCAIANtFGhEuv\n0dLruliPXqOl13WxHr1GS6/rAmA6Jh8uvcdB7+tjNXqPg97Xx2r0Hge9rw+Avk0+XKZAvNAD8UIP\nxAsAx0q4bIjzH/3MsZcAOefFzxp7CQDAhpp0uExpkrHOtS6jRbyMY0qTjHWudRkt4mUcU5pkTGmt\nAPRj0uECAADMw2TDZUrTlqV1rHn7lMXU5WBNadqytI41b5+ymLocrClOMKa4ZgDGNdlwmapDr79g\n7CXAJIMLAJi3SYbLFKct63Ck6Yqpy8Fw8b9wpOmKqcvBMLkAYC4mFy6bEC2mLtO3CdGyCccwd6IF\ngDmZXLiwsNtUxdSFg7DbVMXUBQBYlUmFyyZMW5YOYuoiXtZjkyYVB3Es4mU9TFsAmJtJhcum8ZIx\nerBJIQYAbK7JhMsmTVuO134mKaYuq+Ui/w77maSYuqyWaQsAczSJcNnkaDF1mY5NjpZNPrZNI1oA\nmKtJhAt3OJYJiqkLq3YsExRTFwDgeHQfLps8bVna69RFgIxnDhOJvR6jABmPaQsAc9Z9uLAaooce\niB4A4Fh1HS5zmLYs7TZ1ER7jmcO0ZWm3YxUe4zFtAWDuThh7AUdzz7PeMvYSVu70k68c7bHPf/Qz\n86LDF4/2+FPlgnG1znnxs3LJSy8aexmTs4kB7c8WAPvR9cSFhVVOW0xuOFarnLaY3AAA+yVcAACA\n7gmXzq1jQmLqwn6tY0Ji6gIA7Idw6ZjAoAcCAwDogXCZKVFED0QRALBXwqVTwoIeCAsAoBfCZcbE\nET0QRwDAXgiXDh1kUIgXjuQgg0K8AAC7ES6dERL0QEgAAL0RLogluiCWAICjES4dERD0QEAAAD0S\nLiQRTfRBNAEARyJcOtFDOPSwBsbVQzj0sAYAoD/CpQOCgR4IBgCgZ8JlZL1FS2/r4WD0Fi29rQcA\nGJ9w4U7ECz0QLwDAVsJlRAKBHggEAGAKhAs7ElX0QFQBAEvCZSTCgB4IAwBgKoQLRySu6IG4AgAS\n4TIKQUAPBAEAMCXC5YBNLVqmtl72ZmrRMrX1AgCrJ1zYlXihB+IFAOZNuBygd599ythLAAFAF046\n7dyxlwDAxAgX9sTUhR6ILgCYL+FyQExb6IELf3pg2gLAsRAuB2BTosXUZdo2JVo25TjmSrQAcKyE\nCwAA0D3hsmabMm1ZMnWZpk2bUmza8cyFaQsAx0O4AAAA3RMua7Rp0xamyXSCHpi2AHC8hAv75uVi\n9ECQAcC8CJc1MW2hBy7u6YFpCwCrIFzWYA7RYurSvzlEyxyOcepECwCrIlwAAIDuCZcVm8O0ZcnU\npV9zmkTM6VinxrQFgFUSLgAAQPeEywrNadqyZOrSnzlOIOZ4zL0zbQFg1YTLiswxWpbESz/mfAE/\n52PvjWgBYB2ECwAA0D3hsgJznrYsmbqMz8TB16AHpi0ArMsJYy8gSU4/+cqxl3Bczo9wAVbDhT8A\n7MzE5TiZNNzB12I8Jg138LUAgM0kXI6DC/U78zU5eC7U78zXBAA2TxcvFZuqFx2+eOwlQC556UVj\nLwEAYO1MXAAAgO4JFwAAoHvCBQAA6J5wAQAAuidcAACA7gkXAACge8IFAADonnABAAC6J1wAAIDu\nCRcAAKB7wgUAAOiecAEAALonXAAAgO4JFwAAoHvVWht7DQAAAEdl4gIAAHRPuAAAAN0TLgAAQPeE\nCwAA0D3hAgAAdE+4AAAA3RMuAABA94QLAADQPeECAAB0T7gAAADdEy4AAED3hAsAANA94QIAAHRP\nuAAAAN0TLgAAQPeECwAA0D3hAgAAdE+4AAAA3RMuAABA94QLAADQPeECAAB0T7gAAADdEy4AAED3\n/h9TsuSnHa7vpwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAy4AAACnCAYAAAD35AgmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFwAAoL3/A19pY6ToLwBJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAy4AAACnCAYAAAD35AgmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAD0BJREFUeJzt3GuMbWddx/HfnxYb8MahcksgQUjQ\ngmgaqMglvSggVAuJFoORkmjVGlqS0hJFo4gFRbBqX7QYYwoqSkAraTAcQgNtgdYWDpUXUhEliEa5\nFLABTGsL5fHFXoPjcGbmzJy9Zz9rrc8nOZmzL7P2s05Wp/s7/7V2tdYCAADQs/utewEAAAC7ES4A\nAED3hAsAANA94QIAAHRPuAAAAN0TLgAAQPdmEy5V9eiqes+W+z6xj+0crqpTh7+fXVV3VlUNt19f\nVecdwzZeXVX/tnk9VXVqVd1cVe+vquur6jHD/Y8Z7ruxqm6oqkfusN3HVtVtVfXfVfWMTfdfUVW3\nDn9esen+X62qI1X1oaq6ZK//FoxDVT28qn5/D8+/cafjDDarqgdV1Yu3eeyKqnrIkl7nm36GAzAv\nswmXJbopydOHvz89yW1JnrDp9geOYRtvSHLWlvs+k+Q5rbXTk1ye5LeG+1+S5OrW2plJ/izJS3fY\n7meSPCvJNVvuv6q19kNJnpbk+UPgfHuSn0uycf8vVdW3HsPaGZnW2mdba5duvb+qTljHepicByX5\npnCpqhNaaxe31j6/hjUBMEHCZYuqekNVvbiq7ldV766qp2x5yk1JNqYZP5Dkj5I8o6pOSvKw1tqn\ndnuN1tpnknx9y32fba19Zbh5T5KvDX+/PYs3BklyKMkdVXVSVd1UVd87/Db9Q1V1qLV2V2vtv47y\nev8yfP36sN37ktyd5NNJHjD8uTvJV3dbO+NQVa+rqluGKd0FG7+prqpXVdWfVtU7kvxUVZ01TPpu\nrKo/PMp2XltV7xu29eMHviOMwSVJnjQcQ0e2HF83VtUjq+q7quq9w+2bq+pxSTI890+q6p3DRPih\nw/2XVNWHq+ovh20+evMLVtWjhu+5fvi6lKkOAH07cd0LOGBPqqobd3nOJUmuz2J68t7W2ge3PP6h\nJG+sqvsnaVlMWC5P8tEkR5Kkqp6a5LVH2fZlrbXrd3rxYerxmiTnD3e9J8m7q+r8JCcl+cHW2j3D\n7Tcl+VKSi1trd+6yX6mqn0nyyY24qqrDST6eRcC+prV2727boH9VdXaSRyV5WmutVdVjk7xg01Pu\naa09bzjF8WNJzmitfW7rBKaqnpPkUGvtjKp6YJJbquqdrbV2UPvCKPxBkse31p5ZVa9K8ojW2vOS\npKouGJ7zpSTPba3dW1XPTfKKLCa+SXJ7a+0XqurXsoidv0pyXpLTkjwwySeP8pq/l+TVrbVbq+r5\nSX4lyctXtH8AdGJu4XJba+2ZGzeOdo1La+1/qupNSV6f5BHbPH5Hkp9I8pHW2h1V9fAspjA3Dc+5\nJcmZe13cEENvS/K61to/Dne/Lsmvt9beXlU/neR3klzYWvt4Vf1rkge31v7uGLb9zCQ/m+Sc4fbj\nkvxkksdkES7vq6prW2v/udd1053vS3LDpsC4b8vjG8fLQ5J8sbX2uSRprW193hOTnLEp9k9KcnKS\nLyx9xUzJ0X4ePSjJVcPPym9J8pVNj902fP33JI9N8t1JPtpa+1qSL1fVPx1le09M8ruL9s6JSfZ8\nvSJsVlUXJTk3ySdaaz+/7vUwP47BY+NUsS2q6hFZTDtenUUkHM1NSX45yc3D7U9n8RvtDwzbeOpw\nSsTWPz+8w+veL8lfJLm2tXbt5ofyf28U70jy4OH5z0py/yRfqKrn7bJPTxn259zW2t2btvuV1to9\nw333JPm2nbbDaHw0yRmbbm/973wjUD6f5MEbp9kMx+Bmtye5rrV25nCN1fe31kQLW92b//9LsK0B\nnCQvyuIXPacnuSyLnz8bNk/wKsmnkjyhqk4crsX7nqNs7/YkLxuOzWck+cXjWD+ktXblcDx5w8ha\nOAaPzdwmLjsa3ri9KYtTr26tqrdW1dmttcNbnnpTkkuT3DrcvjnJ87N4w7jrxGWo6hcmOWW49uCC\nJKcm+bEkD6uqFyX5h9baS7M4beyPq+prWYTKBcN54L+d5EezuGblPVX190m+nOTtSR6fxf/4D7fW\nfjPJ1cNLXzv8hvLS1tptw7Uxt2bxZuGG1trH9/HPRmdaa4er6syquiWLa5fets3zWlVdmOQdVXVP\nko8kedmW7TxtmLi0JP+RxSk8sNlnk9xdVX+T5KE5+vTjuiRvqarTs4iObQ2nLb4lyQeT/HMWx929\nWUxqNlyaxQRn45ctb8ziFz8ATFg5XR2AnlTV/VtrX62q78giqB93lFMZAZgZExcAevOKqvqRJN+Z\n5DdECwCJiQsAADACLs4HAAC6J1wAAIDudXGNy0ve/+fOV9vihHPPy33XvHndy1iJN5z+4tr9WQfv\nAade5Djc4vxXXpirL7tq3ctYibs/cqXjkLXr8Th0DM5Lj8dg4jicm2M9DrsIl7k64dydP1l2p8en\nGjUcvPNfeeG+H59q1AAA/REuB2i3UNnvtkQMe7FbqOx3WyIGAFgl4XIAlhksO21fwLCTZQbLTtsX\nMADAKsw6XF54wyl561kfW9n2Vx0s272egGGzVQfLdq8nYMblziNX5tBpF617GQCwrdmGywtvOOUb\nX5cdLwcdLNu9voCZt4MOlu1eX8D0784jV37jq3gBoFc+DnnJ1h0tm/W0Fg7WuqNls57WAgCM1yzD\nZWPast3t/eoxFHpcE6vVYyj0uCYWNqYt290GgF7M9lSxZeo9Dpw6Ng+9x4FTxwCA4zG7ict205X9\nTl16j5bNxrRW9qb3aNlsTGuduu2mK6YuAPRoVuGyrFPCNowxBMa4ZnY2xhAY45qnRpwAMDazCpfd\nLDtseiVe6IF46ZuwAaA3swmXuUQJwG5ECQBjNJtwOVbHGjhjn1qMff0sjH1qMfb1T53AAaAnswiX\nvU5bdnv+VN70T2U/5moqb/qnsh9jsdcYES8A9GLy4bLfU8S2+76pvdmf2v7MxdTe7E9tf3q13wgR\nLwD0YNLh4lPEjs1U92uqpvomf6r71QvxAcDYTTpcjpcL+gEWhA8A6zbZcDFt2Zup799UTH0qMfX9\nWxfRAcAUTDZclsXUBWBBAAGwTpMMF9OW/ZnLfo7VXKYRc9nPgyI2AJiKyYWLCQnAgmgBYEomFy4A\nAMD0TCpcTFsAFkxbAJiaSYXLKvz1VU9e9xIOlOtc+jS36z7mtr9jcei0i9a9BABmbDLhYtoCsGDa\nAsAUTSZcAACA6ZpEuJi2ACyYtgAwVaMPF9ECsCBaAJiy0YcLAAAwfcIFAADonnDZwdw+CnmDj0Tu\ny1w/Gniu+90rH4UMwLqNOlxWfX3LCy788Eq336v7rnnzupfAJldfdtW6l7AWc93v/Vr19S2unwFg\n3UYbLi7KB1gQFQDMwWjDBQAAmI9RhotpC8CCaQsAczHKcAEAAOZldOFi2gKwYNoCwJyMLlwAAID5\nGVW4rGPaMrePRPZRyH2a20cDz21/92Md0xYTHgDWaTTh4hQxgAUBAcAcjSZcAACA+RpFuJi2ACyY\ntgAwV6MIl3Wby3Uurm/p21yu+5jLfo6VcAJgXboPF9MWgAXRAMCcdR0uPUXL1Kcupi3jMPVpxNT3\n73j0FC09rQWA+eg2XHqKlg1TjRfRMi5TfXM/1f1ahh5Docc1ATBt3YYLAADAhi7Dpcdpy4apTV1M\nW8ZpatOJqe3PMvU82eh5bQBMT5fh0rupxItoGbepvNmfyn7MlXgB4KB0Fy49T1s2G3u8iJZpGPub\n/rGvf9XGEgVjWScA43biuhew1VvP+ti6l3DMTrjqyeteAjBhh067aN1LAIBudDdxYfVMW+iBaQsA\nsBfC5TiMMQDGuGZ2NsYAGOOaAYD1Ei7HaUwhMKa1sjdjCoExrRUA6Ed317iM0UYQnHDueUvb5rvu\nOjnPfeAXl7ItwTIPG0Fw/isvXNo2Lz/nlLz8b5dz3ZlgAQCOh3BZovuuefOe4+Vdd52858f2EjSi\nZX6uvuyqPcfL5eds/2l+2z22l6ARLQDA8XKq2JIdayi8666Td4yW3b53mWtheo41FC4/55Qdo2W3\n713mWgAAdmLisgI7nTq231jZbjtHm74IFpKdTx3bb6xst52jTV8ECwCwTMJlhTYHzLKCZavNASNY\nOJrNAbOsYNlqc8AIFgBgFZwqdgBWFS0H/RqM26qi5aBfAwCYJ+GyYtedffEkX4txufPIlZN8LQBg\nPoTLCq0jJMQLW60jJMQLALBswmVF1hkQ4oUN6wwI8QIALJNwWYEewqGHNbBePYRDD2sAAKZBuAAA\nAN0TLkvW06Sjp7VwsHqadPS0FgBgvIQLAADQPeGyRD1OOHpcE6vV44SjxzUBAOMiXAAAgO4JlyXp\nebLR89pYrp4nGz2vDQDon3ABAAC6J1wAAIDuCRcAAKB7wgUAAOiecAEAALonXJZgDJ/aNYY1cnzG\n8KldY1gjANAn4bIEzz58xbqXsKsxrJHjc+i0i9a9hF2NYY0AQJ+ECwAA0D3hAgAAdE+4AAAA3RMu\nAABA94TLkvR88XvPa2O5er74vee1AQD9Ey4AAED3hMsS9TjZ6HFNrFaPk40e1wQAjItwAQAAuidc\nlqynCUdPa+Fg9TTh6GktAMB4CRcAAKB7wmUFeph09LAG1quHSUcPawAApkG4rMg6w0G0sGGd4SBa\nAIBlEi4rtI6AEC1stY6AEC0AwLIJlxU7yJAQLWznIENCtAAAqyBcDsBBBIVoYTcHERSiBQBYlRPX\nvYC52AiL686+eCXbhWOxERZ3HrlyJdsFAFgV4XLAlhUwgoXjsayAESwAwEFxqtiaPPvwFfuOD9HC\nshw67aJ9x4doAQAOkonLmm0XIdedfbFA4cBsFyF3HrlSoAAAXTBx6ZRooQeiBQDohXABAAC6J1wA\nAIDuCRcAAKB7wgUAAOiecAEAALonXAAAgO4JFwAAoHvCBQAA6J5wAQAAuidcAACA7gkXAACge8IF\nAADonnABAAC6J1wAAIDuCRcAAKB7wgUAAOiecAEAALonXAAAgO4JFwAAoHvCBQAA6J5wAQAAuidc\nAACA7gkXAACge8IFAADonnABAAC6J1wAAIDuVWtt3WsAAADYkYkLAADQPeECAAB0T7gAAADdEy4A\nAED3hAsAANA94QIAAHRPuAAAAN0TLgAAQPeECwAA0D3hAgAAdE+4AAAA3RMuAABA94QLAADQPeEC\nAAB0T7gAAADdEy4AAED3hAsAANA94QIAAHRPuAAAAN0TLgAAQPeECwAA0D3hAgAAdE+4AAAA3ftf\nKCJWod10Nn4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAy4AAACnCAYAAAD35AgmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Which weights to start with?\n",
    "# init_with = \"last\"  # imagenet, coco, or last\n",
    "\n",
    "# if init_with == \"imagenet\":\n",
    "#     model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "# elif init_with == \"coco\":\n",
    "#     # Load weights trained on MS COCO, but skip layers that\n",
    "#     # are different due to the different number of classes\n",
    "#     # See README for instructions to download the COCO weights\n",
    "#     model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "#                        exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "#                                 \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "# elif init_with == \"last\":\n",
    "#     # Load the last model you trained and continue training\n",
    "#     model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.02\n",
      "\n",
      "Checkpoint Path: /scratch/ruian/Mask_RCNN/logs/shapes20190110T1607/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "('In model: ', 'rpn_model')\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n",
      "WARNING:tensorflow:From /scratch/ruian/Mask_RCNN/mrcnn/model.py:2214: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_image to have shape (None, None, 1) but got array with shape (128, 128, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6bc3e6085787>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             layers='heads')\n\u001b[0m",
      "\u001b[0;32m/scratch/ruian/Mask_RCNN/mrcnn/model.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)\u001b[0m\n\u001b[1;32m   2416\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m             \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m         )\n\u001b[1;32m   2420\u001b[0m         '''\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training_generator.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training_utils.pyc\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_image to have shape (None, None, 1) but got array with shape (128, 128, 3)"
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=10, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=20, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "# model_path = model.find_last()\n",
    "# trained for 20 epochs in total\n",
    "model_path=\"/data/dayajun/sw/tmp/Mask_RCNN/logs/shapes20180924T1723/mask_rcnn_shapes_0301.h5\"\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))\n",
    "\n",
    "\n",
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
